{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c98305e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b51b72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = 'archive/movie_conversations.txt'\n",
    "corpus_movie_lines = 'archive/movie_lines.txt'\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1944bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    conv = c.readlines()\n",
    "with open(corpus_movie_lines, 'r') as l:\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "378f1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cbc92270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char  # space is also a character\n",
    "    return no_punct.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8db01aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i==len(ids)-1:\n",
    "            break\n",
    "        \n",
    "        first = remove_punc(lines_dic[ids[i]].strip())      \n",
    "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4f5ed441",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0f831cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq = 5\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k:v+1 for v,k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e6da66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 18238\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words are: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "14da6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WORDMAP_corpus.json', 'w') as j:\n",
    "    json.dump(word_map,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "45615d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_questions(words, word_map):\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "32e63271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reply(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "132845eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    ques = encode_questions(pair[0], word_map)\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append([ques,ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "75ceba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a2aa6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = json.load(open('pairs_encoded.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "        return question, reply\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bbd86c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Dataset(), batch_size = 100, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8f0ec7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    \n",
    "    def subsequent_masks(size):\n",
    "        mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = (question!=0).to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
    "    reply_input_mask = reply_input_mask & subsequent_masks(reply_input.size(-1)).type_as(reply_input_mask.data)\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "119ac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, encoded_words):\n",
    "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4dd232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3e48ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "50064110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b94fc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "350803e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c61db3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f9235a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "867668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 25\n",
    "\n",
    "with open('WORDMAP_corpus.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6474235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    \n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        \n",
    "        samples = question.shape[0]\n",
    "\n",
    "        # Move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "\n",
    "        # Create mask and add dimensions\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "\n",
    "        # Get the transformer outputs\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        \n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ac7ac110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    rev_word_map = {v:k for k,v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['<start>']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[0]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        # Construct Sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4c83fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/2217]\tLoss: 8.677\n",
      "Epoch [0][100/2217]\tLoss: 7.722\n",
      "Epoch [0][200/2217]\tLoss: 7.110\n",
      "Epoch [0][300/2217]\tLoss: 6.587\n",
      "Epoch [0][400/2217]\tLoss: 6.227\n",
      "Epoch [0][500/2217]\tLoss: 5.977\n",
      "Epoch [0][600/2217]\tLoss: 5.795\n",
      "Epoch [0][700/2217]\tLoss: 5.653\n",
      "Epoch [0][800/2217]\tLoss: 5.538\n",
      "Epoch [0][900/2217]\tLoss: 5.445\n",
      "Epoch [0][1000/2217]\tLoss: 5.367\n",
      "Epoch [0][1100/2217]\tLoss: 5.300\n",
      "Epoch [0][1200/2217]\tLoss: 5.241\n",
      "Epoch [0][1300/2217]\tLoss: 5.189\n",
      "Epoch [0][1400/2217]\tLoss: 5.144\n",
      "Epoch [0][1500/2217]\tLoss: 5.104\n",
      "Epoch [0][1600/2217]\tLoss: 5.069\n",
      "Epoch [0][1700/2217]\tLoss: 5.036\n",
      "Epoch [0][1800/2217]\tLoss: 5.005\n",
      "Epoch [0][1900/2217]\tLoss: 4.977\n",
      "Epoch [0][2000/2217]\tLoss: 4.952\n",
      "Epoch [0][2100/2217]\tLoss: 4.929\n",
      "Epoch [0][2200/2217]\tLoss: 4.906\n",
      "Epoch [1][0/2217]\tLoss: 4.371\n",
      "Epoch [1][100/2217]\tLoss: 4.402\n",
      "Epoch [1][200/2217]\tLoss: 4.394\n",
      "Epoch [1][300/2217]\tLoss: 4.397\n",
      "Epoch [1][400/2217]\tLoss: 4.399\n",
      "Epoch [1][500/2217]\tLoss: 4.397\n",
      "Epoch [1][600/2217]\tLoss: 4.399\n",
      "Epoch [1][700/2217]\tLoss: 4.400\n",
      "Epoch [1][800/2217]\tLoss: 4.399\n",
      "Epoch [1][900/2217]\tLoss: 4.397\n",
      "Epoch [1][1000/2217]\tLoss: 4.397\n",
      "Epoch [1][1100/2217]\tLoss: 4.396\n",
      "Epoch [1][1200/2217]\tLoss: 4.395\n",
      "Epoch [1][1300/2217]\tLoss: 4.394\n",
      "Epoch [1][1400/2217]\tLoss: 4.394\n",
      "Epoch [1][1500/2217]\tLoss: 4.395\n",
      "Epoch [1][1600/2217]\tLoss: 4.394\n",
      "Epoch [1][1700/2217]\tLoss: 4.394\n",
      "Epoch [1][1800/2217]\tLoss: 4.394\n",
      "Epoch [1][1900/2217]\tLoss: 4.394\n",
      "Epoch [1][2000/2217]\tLoss: 4.393\n",
      "Epoch [1][2100/2217]\tLoss: 4.392\n",
      "Epoch [1][2200/2217]\tLoss: 4.392\n",
      "Epoch [2][0/2217]\tLoss: 4.332\n",
      "Epoch [2][100/2217]\tLoss: 4.307\n",
      "Epoch [2][200/2217]\tLoss: 4.315\n",
      "Epoch [2][300/2217]\tLoss: 4.319\n",
      "Epoch [2][400/2217]\tLoss: 4.318\n",
      "Epoch [2][500/2217]\tLoss: 4.320\n",
      "Epoch [2][600/2217]\tLoss: 4.320\n",
      "Epoch [2][700/2217]\tLoss: 4.321\n",
      "Epoch [2][800/2217]\tLoss: 4.321\n",
      "Epoch [2][900/2217]\tLoss: 4.321\n",
      "Epoch [2][1000/2217]\tLoss: 4.321\n",
      "Epoch [2][1100/2217]\tLoss: 4.320\n",
      "Epoch [2][1200/2217]\tLoss: 4.319\n",
      "Epoch [2][1300/2217]\tLoss: 4.319\n",
      "Epoch [2][1400/2217]\tLoss: 4.317\n",
      "Epoch [2][1500/2217]\tLoss: 4.317\n",
      "Epoch [2][1600/2217]\tLoss: 4.316\n",
      "Epoch [2][1700/2217]\tLoss: 4.314\n",
      "Epoch [2][1800/2217]\tLoss: 4.314\n",
      "Epoch [2][1900/2217]\tLoss: 4.313\n",
      "Epoch [2][2000/2217]\tLoss: 4.312\n",
      "Epoch [2][2100/2217]\tLoss: 4.310\n",
      "Epoch [2][2200/2217]\tLoss: 4.310\n",
      "Epoch [3][0/2217]\tLoss: 4.236\n",
      "Epoch [3][100/2217]\tLoss: 4.214\n",
      "Epoch [3][200/2217]\tLoss: 4.213\n",
      "Epoch [3][300/2217]\tLoss: 4.223\n",
      "Epoch [3][400/2217]\tLoss: 4.225\n",
      "Epoch [3][500/2217]\tLoss: 4.227\n",
      "Epoch [3][600/2217]\tLoss: 4.230\n",
      "Epoch [3][700/2217]\tLoss: 4.233\n",
      "Epoch [3][800/2217]\tLoss: 4.233\n",
      "Epoch [3][900/2217]\tLoss: 4.233\n",
      "Epoch [3][1000/2217]\tLoss: 4.232\n",
      "Epoch [3][1100/2217]\tLoss: 4.231\n",
      "Epoch [3][1200/2217]\tLoss: 4.230\n",
      "Epoch [3][1300/2217]\tLoss: 4.231\n",
      "Epoch [3][1400/2217]\tLoss: 4.232\n",
      "Epoch [3][1500/2217]\tLoss: 4.232\n",
      "Epoch [3][1600/2217]\tLoss: 4.232\n",
      "Epoch [3][1700/2217]\tLoss: 4.233\n",
      "Epoch [3][1800/2217]\tLoss: 4.232\n",
      "Epoch [3][1900/2217]\tLoss: 4.233\n",
      "Epoch [3][2000/2217]\tLoss: 4.232\n",
      "Epoch [3][2100/2217]\tLoss: 4.232\n",
      "Epoch [3][2200/2217]\tLoss: 4.232\n",
      "Epoch [4][0/2217]\tLoss: 4.199\n",
      "Epoch [4][100/2217]\tLoss: 4.161\n",
      "Epoch [4][200/2217]\tLoss: 4.158\n",
      "Epoch [4][300/2217]\tLoss: 4.158\n",
      "Epoch [4][400/2217]\tLoss: 4.163\n",
      "Epoch [4][500/2217]\tLoss: 4.165\n",
      "Epoch [4][600/2217]\tLoss: 4.166\n",
      "Epoch [4][700/2217]\tLoss: 4.166\n",
      "Epoch [4][800/2217]\tLoss: 4.165\n",
      "Epoch [4][900/2217]\tLoss: 4.167\n",
      "Epoch [4][1000/2217]\tLoss: 4.167\n",
      "Epoch [4][1100/2217]\tLoss: 4.168\n",
      "Epoch [4][1200/2217]\tLoss: 4.169\n",
      "Epoch [4][1300/2217]\tLoss: 4.170\n",
      "Epoch [4][1400/2217]\tLoss: 4.171\n",
      "Epoch [4][1500/2217]\tLoss: 4.172\n",
      "Epoch [4][1600/2217]\tLoss: 4.173\n",
      "Epoch [4][1700/2217]\tLoss: 4.172\n",
      "Epoch [4][1800/2217]\tLoss: 4.173\n",
      "Epoch [4][1900/2217]\tLoss: 4.173\n",
      "Epoch [4][2000/2217]\tLoss: 4.173\n",
      "Epoch [4][2100/2217]\tLoss: 4.173\n",
      "Epoch [4][2200/2217]\tLoss: 4.174\n",
      "Epoch [5][0/2217]\tLoss: 4.138\n",
      "Epoch [5][100/2217]\tLoss: 4.106\n",
      "Epoch [5][200/2217]\tLoss: 4.108\n",
      "Epoch [5][300/2217]\tLoss: 4.107\n",
      "Epoch [5][400/2217]\tLoss: 4.108\n",
      "Epoch [5][500/2217]\tLoss: 4.109\n",
      "Epoch [5][600/2217]\tLoss: 4.111\n",
      "Epoch [5][700/2217]\tLoss: 4.114\n",
      "Epoch [5][800/2217]\tLoss: 4.116\n",
      "Epoch [5][900/2217]\tLoss: 4.119\n",
      "Epoch [5][1000/2217]\tLoss: 4.119\n",
      "Epoch [5][1100/2217]\tLoss: 4.119\n",
      "Epoch [5][1200/2217]\tLoss: 4.121\n",
      "Epoch [5][1300/2217]\tLoss: 4.121\n",
      "Epoch [5][1400/2217]\tLoss: 4.121\n",
      "Epoch [5][1500/2217]\tLoss: 4.123\n",
      "Epoch [5][1600/2217]\tLoss: 4.124\n",
      "Epoch [5][1700/2217]\tLoss: 4.124\n",
      "Epoch [5][1800/2217]\tLoss: 4.125\n",
      "Epoch [5][1900/2217]\tLoss: 4.125\n",
      "Epoch [5][2000/2217]\tLoss: 4.125\n",
      "Epoch [5][2100/2217]\tLoss: 4.126\n",
      "Epoch [5][2200/2217]\tLoss: 4.126\n",
      "Epoch [6][0/2217]\tLoss: 4.025\n",
      "Epoch [6][100/2217]\tLoss: 4.056\n",
      "Epoch [6][200/2217]\tLoss: 4.058\n",
      "Epoch [6][300/2217]\tLoss: 4.062\n",
      "Epoch [6][400/2217]\tLoss: 4.058\n",
      "Epoch [6][500/2217]\tLoss: 4.056\n",
      "Epoch [6][600/2217]\tLoss: 4.060\n",
      "Epoch [6][700/2217]\tLoss: 4.064\n",
      "Epoch [6][800/2217]\tLoss: 4.065\n",
      "Epoch [6][900/2217]\tLoss: 4.066\n",
      "Epoch [6][1000/2217]\tLoss: 4.069\n",
      "Epoch [6][1100/2217]\tLoss: 4.070\n",
      "Epoch [6][1200/2217]\tLoss: 4.070\n",
      "Epoch [6][1300/2217]\tLoss: 4.071\n",
      "Epoch [6][1400/2217]\tLoss: 4.074\n",
      "Epoch [6][1500/2217]\tLoss: 4.075\n",
      "Epoch [6][1600/2217]\tLoss: 4.076\n",
      "Epoch [6][1700/2217]\tLoss: 4.077\n",
      "Epoch [6][1800/2217]\tLoss: 4.078\n",
      "Epoch [6][1900/2217]\tLoss: 4.080\n",
      "Epoch [6][2000/2217]\tLoss: 4.080\n",
      "Epoch [6][2100/2217]\tLoss: 4.081\n",
      "Epoch [6][2200/2217]\tLoss: 4.081\n",
      "Epoch [7][0/2217]\tLoss: 4.028\n",
      "Epoch [7][100/2217]\tLoss: 4.005\n",
      "Epoch [7][200/2217]\tLoss: 4.008\n",
      "Epoch [7][300/2217]\tLoss: 4.010\n",
      "Epoch [7][400/2217]\tLoss: 4.016\n",
      "Epoch [7][500/2217]\tLoss: 4.017\n",
      "Epoch [7][600/2217]\tLoss: 4.018\n",
      "Epoch [7][700/2217]\tLoss: 4.021\n",
      "Epoch [7][800/2217]\tLoss: 4.023\n",
      "Epoch [7][900/2217]\tLoss: 4.024\n",
      "Epoch [7][1000/2217]\tLoss: 4.026\n",
      "Epoch [7][1100/2217]\tLoss: 4.027\n",
      "Epoch [7][1200/2217]\tLoss: 4.029\n",
      "Epoch [7][1300/2217]\tLoss: 4.030\n",
      "Epoch [7][1400/2217]\tLoss: 4.033\n",
      "Epoch [7][1500/2217]\tLoss: 4.036\n",
      "Epoch [7][1600/2217]\tLoss: 4.036\n",
      "Epoch [7][1700/2217]\tLoss: 4.037\n",
      "Epoch [7][1800/2217]\tLoss: 4.038\n",
      "Epoch [7][1900/2217]\tLoss: 4.039\n",
      "Epoch [7][2000/2217]\tLoss: 4.040\n",
      "Epoch [7][2100/2217]\tLoss: 4.040\n",
      "Epoch [7][2200/2217]\tLoss: 4.040\n",
      "Epoch [8][0/2217]\tLoss: 3.880\n",
      "Epoch [8][100/2217]\tLoss: 3.961\n",
      "Epoch [8][200/2217]\tLoss: 3.966\n",
      "Epoch [8][300/2217]\tLoss: 3.970\n",
      "Epoch [8][400/2217]\tLoss: 3.968\n",
      "Epoch [8][500/2217]\tLoss: 3.973\n",
      "Epoch [8][600/2217]\tLoss: 3.977\n",
      "Epoch [8][700/2217]\tLoss: 3.980\n",
      "Epoch [8][800/2217]\tLoss: 3.982\n",
      "Epoch [8][900/2217]\tLoss: 3.984\n",
      "Epoch [8][1000/2217]\tLoss: 3.988\n",
      "Epoch [8][1100/2217]\tLoss: 3.991\n",
      "Epoch [8][1200/2217]\tLoss: 3.993\n",
      "Epoch [8][1300/2217]\tLoss: 3.993\n",
      "Epoch [8][1400/2217]\tLoss: 3.996\n",
      "Epoch [8][1500/2217]\tLoss: 3.997\n",
      "Epoch [8][1600/2217]\tLoss: 3.998\n",
      "Epoch [8][1700/2217]\tLoss: 3.999\n",
      "Epoch [8][1800/2217]\tLoss: 4.001\n",
      "Epoch [8][1900/2217]\tLoss: 4.002\n",
      "Epoch [8][2000/2217]\tLoss: 4.003\n",
      "Epoch [8][2100/2217]\tLoss: 4.004\n",
      "Epoch [8][2200/2217]\tLoss: 4.004\n",
      "Epoch [9][0/2217]\tLoss: 3.865\n",
      "Epoch [9][100/2217]\tLoss: 3.916\n",
      "Epoch [9][200/2217]\tLoss: 3.924\n",
      "Epoch [9][300/2217]\tLoss: 3.924\n",
      "Epoch [9][400/2217]\tLoss: 3.926\n",
      "Epoch [9][500/2217]\tLoss: 3.932\n",
      "Epoch [9][600/2217]\tLoss: 3.936\n",
      "Epoch [9][700/2217]\tLoss: 3.942\n",
      "Epoch [9][800/2217]\tLoss: 3.944\n",
      "Epoch [9][900/2217]\tLoss: 3.949\n",
      "Epoch [9][1000/2217]\tLoss: 3.952\n",
      "Epoch [9][1100/2217]\tLoss: 3.953\n",
      "Epoch [9][1200/2217]\tLoss: 3.954\n",
      "Epoch [9][1300/2217]\tLoss: 3.957\n",
      "Epoch [9][1400/2217]\tLoss: 3.959\n",
      "Epoch [9][1500/2217]\tLoss: 3.962\n",
      "Epoch [9][1600/2217]\tLoss: 3.963\n",
      "Epoch [9][1700/2217]\tLoss: 3.965\n",
      "Epoch [9][1800/2217]\tLoss: 3.967\n",
      "Epoch [9][1900/2217]\tLoss: 3.967\n",
      "Epoch [9][2000/2217]\tLoss: 3.968\n",
      "Epoch [9][2100/2217]\tLoss: 3.969\n",
      "Epoch [9][2200/2217]\tLoss: 3.970\n",
      "Epoch [10][0/2217]\tLoss: 3.843\n",
      "Epoch [10][100/2217]\tLoss: 3.883\n",
      "Epoch [10][200/2217]\tLoss: 3.891\n",
      "Epoch [10][300/2217]\tLoss: 3.897\n",
      "Epoch [10][400/2217]\tLoss: 3.903\n",
      "Epoch [10][500/2217]\tLoss: 3.909\n",
      "Epoch [10][600/2217]\tLoss: 3.912\n",
      "Epoch [10][700/2217]\tLoss: 3.912\n",
      "Epoch [10][800/2217]\tLoss: 3.917\n",
      "Epoch [10][900/2217]\tLoss: 3.921\n",
      "Epoch [10][1000/2217]\tLoss: 3.924\n",
      "Epoch [10][1100/2217]\tLoss: 3.924\n",
      "Epoch [10][1200/2217]\tLoss: 3.926\n",
      "Epoch [10][1300/2217]\tLoss: 3.928\n",
      "Epoch [10][1400/2217]\tLoss: 3.930\n",
      "Epoch [10][1500/2217]\tLoss: 3.930\n",
      "Epoch [10][1600/2217]\tLoss: 3.932\n",
      "Epoch [10][1700/2217]\tLoss: 3.933\n",
      "Epoch [10][1800/2217]\tLoss: 3.934\n",
      "Epoch [10][1900/2217]\tLoss: 3.936\n",
      "Epoch [10][2000/2217]\tLoss: 3.937\n",
      "Epoch [10][2100/2217]\tLoss: 3.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][2200/2217]\tLoss: 3.939\n",
      "Epoch [11][0/2217]\tLoss: 3.882\n",
      "Epoch [11][100/2217]\tLoss: 3.869\n",
      "Epoch [11][200/2217]\tLoss: 3.868\n",
      "Epoch [11][300/2217]\tLoss: 3.875\n",
      "Epoch [11][400/2217]\tLoss: 3.871\n",
      "Epoch [11][500/2217]\tLoss: 3.873\n",
      "Epoch [11][600/2217]\tLoss: 3.875\n",
      "Epoch [11][700/2217]\tLoss: 3.878\n",
      "Epoch [11][800/2217]\tLoss: 3.882\n",
      "Epoch [11][900/2217]\tLoss: 3.887\n",
      "Epoch [11][1000/2217]\tLoss: 3.889\n",
      "Epoch [11][1100/2217]\tLoss: 3.892\n",
      "Epoch [11][1200/2217]\tLoss: 3.893\n",
      "Epoch [11][1300/2217]\tLoss: 3.895\n",
      "Epoch [11][1400/2217]\tLoss: 3.897\n",
      "Epoch [11][1500/2217]\tLoss: 3.899\n",
      "Epoch [11][1600/2217]\tLoss: 3.900\n",
      "Epoch [11][1700/2217]\tLoss: 3.902\n",
      "Epoch [11][1800/2217]\tLoss: 3.903\n",
      "Epoch [11][1900/2217]\tLoss: 3.905\n",
      "Epoch [11][2000/2217]\tLoss: 3.906\n",
      "Epoch [11][2100/2217]\tLoss: 3.907\n",
      "Epoch [11][2200/2217]\tLoss: 3.908\n",
      "Epoch [12][0/2217]\tLoss: 3.795\n",
      "Epoch [12][100/2217]\tLoss: 3.827\n",
      "Epoch [12][200/2217]\tLoss: 3.835\n",
      "Epoch [12][300/2217]\tLoss: 3.840\n",
      "Epoch [12][400/2217]\tLoss: 3.840\n",
      "Epoch [12][500/2217]\tLoss: 3.841\n",
      "Epoch [12][600/2217]\tLoss: 3.842\n",
      "Epoch [12][700/2217]\tLoss: 3.846\n",
      "Epoch [12][800/2217]\tLoss: 3.852\n",
      "Epoch [12][900/2217]\tLoss: 3.854\n",
      "Epoch [12][1000/2217]\tLoss: 3.858\n",
      "Epoch [12][1100/2217]\tLoss: 3.861\n",
      "Epoch [12][1200/2217]\tLoss: 3.862\n",
      "Epoch [12][1300/2217]\tLoss: 3.864\n",
      "Epoch [12][1400/2217]\tLoss: 3.866\n",
      "Epoch [12][1500/2217]\tLoss: 3.869\n",
      "Epoch [12][1600/2217]\tLoss: 3.870\n",
      "Epoch [12][1700/2217]\tLoss: 3.872\n",
      "Epoch [12][1800/2217]\tLoss: 3.873\n",
      "Epoch [12][1900/2217]\tLoss: 3.875\n",
      "Epoch [12][2000/2217]\tLoss: 3.876\n",
      "Epoch [12][2100/2217]\tLoss: 3.877\n",
      "Epoch [12][2200/2217]\tLoss: 3.878\n",
      "Epoch [13][0/2217]\tLoss: 3.717\n",
      "Epoch [13][100/2217]\tLoss: 3.798\n",
      "Epoch [13][200/2217]\tLoss: 3.797\n",
      "Epoch [13][300/2217]\tLoss: 3.805\n",
      "Epoch [13][400/2217]\tLoss: 3.807\n",
      "Epoch [13][500/2217]\tLoss: 3.812\n",
      "Epoch [13][600/2217]\tLoss: 3.812\n",
      "Epoch [13][700/2217]\tLoss: 3.814\n",
      "Epoch [13][800/2217]\tLoss: 3.819\n",
      "Epoch [13][900/2217]\tLoss: 3.824\n",
      "Epoch [13][1000/2217]\tLoss: 3.825\n",
      "Epoch [13][1100/2217]\tLoss: 3.829\n",
      "Epoch [13][1200/2217]\tLoss: 3.832\n",
      "Epoch [13][1300/2217]\tLoss: 3.834\n",
      "Epoch [13][1400/2217]\tLoss: 3.836\n",
      "Epoch [13][1500/2217]\tLoss: 3.838\n",
      "Epoch [13][1600/2217]\tLoss: 3.840\n",
      "Epoch [13][1700/2217]\tLoss: 3.842\n",
      "Epoch [13][1800/2217]\tLoss: 3.844\n",
      "Epoch [13][1900/2217]\tLoss: 3.845\n",
      "Epoch [13][2000/2217]\tLoss: 3.847\n",
      "Epoch [13][2100/2217]\tLoss: 3.849\n",
      "Epoch [13][2200/2217]\tLoss: 3.850\n",
      "Epoch [14][0/2217]\tLoss: 3.686\n",
      "Epoch [14][100/2217]\tLoss: 3.787\n",
      "Epoch [14][200/2217]\tLoss: 3.783\n",
      "Epoch [14][300/2217]\tLoss: 3.783\n",
      "Epoch [14][400/2217]\tLoss: 3.788\n",
      "Epoch [14][500/2217]\tLoss: 3.792\n",
      "Epoch [14][600/2217]\tLoss: 3.789\n",
      "Epoch [14][700/2217]\tLoss: 3.791\n",
      "Epoch [14][800/2217]\tLoss: 3.795\n",
      "Epoch [14][900/2217]\tLoss: 3.799\n",
      "Epoch [14][1000/2217]\tLoss: 3.801\n",
      "Epoch [14][1100/2217]\tLoss: 3.803\n",
      "Epoch [14][1200/2217]\tLoss: 3.805\n",
      "Epoch [14][1300/2217]\tLoss: 3.807\n",
      "Epoch [14][1400/2217]\tLoss: 3.808\n",
      "Epoch [14][1500/2217]\tLoss: 3.810\n",
      "Epoch [14][1600/2217]\tLoss: 3.812\n",
      "Epoch [14][1700/2217]\tLoss: 3.814\n",
      "Epoch [14][1800/2217]\tLoss: 3.816\n",
      "Epoch [14][1900/2217]\tLoss: 3.818\n",
      "Epoch [14][2000/2217]\tLoss: 3.820\n",
      "Epoch [14][2100/2217]\tLoss: 3.821\n",
      "Epoch [14][2200/2217]\tLoss: 3.823\n",
      "Epoch [15][0/2217]\tLoss: 3.838\n",
      "Epoch [15][100/2217]\tLoss: 3.740\n",
      "Epoch [15][200/2217]\tLoss: 3.746\n",
      "Epoch [15][300/2217]\tLoss: 3.754\n",
      "Epoch [15][400/2217]\tLoss: 3.755\n",
      "Epoch [15][500/2217]\tLoss: 3.759\n",
      "Epoch [15][600/2217]\tLoss: 3.760\n",
      "Epoch [15][700/2217]\tLoss: 3.765\n",
      "Epoch [15][800/2217]\tLoss: 3.765\n",
      "Epoch [15][900/2217]\tLoss: 3.767\n",
      "Epoch [15][1000/2217]\tLoss: 3.769\n",
      "Epoch [15][1100/2217]\tLoss: 3.773\n",
      "Epoch [15][1200/2217]\tLoss: 3.776\n",
      "Epoch [15][1300/2217]\tLoss: 3.779\n",
      "Epoch [15][1400/2217]\tLoss: 3.782\n",
      "Epoch [15][1500/2217]\tLoss: 3.784\n",
      "Epoch [15][1600/2217]\tLoss: 3.786\n",
      "Epoch [15][1700/2217]\tLoss: 3.788\n",
      "Epoch [15][1800/2217]\tLoss: 3.791\n",
      "Epoch [15][1900/2217]\tLoss: 3.792\n",
      "Epoch [15][2000/2217]\tLoss: 3.793\n",
      "Epoch [15][2100/2217]\tLoss: 3.795\n",
      "Epoch [15][2200/2217]\tLoss: 3.797\n",
      "Epoch [16][0/2217]\tLoss: 3.727\n",
      "Epoch [16][100/2217]\tLoss: 3.712\n",
      "Epoch [16][200/2217]\tLoss: 3.715\n",
      "Epoch [16][300/2217]\tLoss: 3.723\n",
      "Epoch [16][400/2217]\tLoss: 3.728\n",
      "Epoch [16][500/2217]\tLoss: 3.733\n",
      "Epoch [16][600/2217]\tLoss: 3.736\n",
      "Epoch [16][700/2217]\tLoss: 3.738\n",
      "Epoch [16][800/2217]\tLoss: 3.742\n",
      "Epoch [16][900/2217]\tLoss: 3.745\n",
      "Epoch [16][1000/2217]\tLoss: 3.748\n",
      "Epoch [16][1100/2217]\tLoss: 3.751\n",
      "Epoch [16][1200/2217]\tLoss: 3.753\n",
      "Epoch [16][1300/2217]\tLoss: 3.755\n",
      "Epoch [16][1400/2217]\tLoss: 3.757\n",
      "Epoch [16][1500/2217]\tLoss: 3.759\n",
      "Epoch [16][1600/2217]\tLoss: 3.761\n",
      "Epoch [16][1700/2217]\tLoss: 3.763\n",
      "Epoch [16][1800/2217]\tLoss: 3.766\n",
      "Epoch [16][1900/2217]\tLoss: 3.768\n",
      "Epoch [16][2000/2217]\tLoss: 3.770\n",
      "Epoch [16][2100/2217]\tLoss: 3.771\n",
      "Epoch [16][2200/2217]\tLoss: 3.773\n",
      "Epoch [17][0/2217]\tLoss: 3.611\n",
      "Epoch [17][100/2217]\tLoss: 3.701\n",
      "Epoch [17][200/2217]\tLoss: 3.702\n",
      "Epoch [17][300/2217]\tLoss: 3.705\n",
      "Epoch [17][400/2217]\tLoss: 3.705\n",
      "Epoch [17][500/2217]\tLoss: 3.711\n",
      "Epoch [17][600/2217]\tLoss: 3.716\n",
      "Epoch [17][700/2217]\tLoss: 3.718\n",
      "Epoch [17][800/2217]\tLoss: 3.721\n",
      "Epoch [17][900/2217]\tLoss: 3.723\n",
      "Epoch [17][1000/2217]\tLoss: 3.727\n",
      "Epoch [17][1100/2217]\tLoss: 3.730\n",
      "Epoch [17][1200/2217]\tLoss: 3.733\n",
      "Epoch [17][1300/2217]\tLoss: 3.734\n",
      "Epoch [17][1400/2217]\tLoss: 3.737\n",
      "Epoch [17][1500/2217]\tLoss: 3.739\n",
      "Epoch [17][1600/2217]\tLoss: 3.740\n",
      "Epoch [17][1700/2217]\tLoss: 3.742\n",
      "Epoch [17][1800/2217]\tLoss: 3.744\n",
      "Epoch [17][1900/2217]\tLoss: 3.746\n",
      "Epoch [17][2000/2217]\tLoss: 3.748\n",
      "Epoch [17][2100/2217]\tLoss: 3.749\n",
      "Epoch [17][2200/2217]\tLoss: 3.750\n",
      "Epoch [18][0/2217]\tLoss: 3.757\n",
      "Epoch [18][100/2217]\tLoss: 3.688\n",
      "Epoch [18][200/2217]\tLoss: 3.681\n",
      "Epoch [18][300/2217]\tLoss: 3.677\n",
      "Epoch [18][400/2217]\tLoss: 3.680\n",
      "Epoch [18][500/2217]\tLoss: 3.685\n",
      "Epoch [18][600/2217]\tLoss: 3.688\n",
      "Epoch [18][700/2217]\tLoss: 3.690\n",
      "Epoch [18][800/2217]\tLoss: 3.696\n",
      "Epoch [18][900/2217]\tLoss: 3.699\n",
      "Epoch [18][1000/2217]\tLoss: 3.703\n",
      "Epoch [18][1100/2217]\tLoss: 3.706\n",
      "Epoch [18][1200/2217]\tLoss: 3.708\n",
      "Epoch [18][1300/2217]\tLoss: 3.710\n",
      "Epoch [18][1400/2217]\tLoss: 3.712\n",
      "Epoch [18][1500/2217]\tLoss: 3.714\n",
      "Epoch [18][1600/2217]\tLoss: 3.716\n",
      "Epoch [18][1700/2217]\tLoss: 3.719\n",
      "Epoch [18][1800/2217]\tLoss: 3.721\n",
      "Epoch [18][1900/2217]\tLoss: 3.723\n",
      "Epoch [18][2000/2217]\tLoss: 3.725\n",
      "Epoch [18][2100/2217]\tLoss: 3.727\n",
      "Epoch [18][2200/2217]\tLoss: 3.729\n",
      "Epoch [19][0/2217]\tLoss: 3.405\n",
      "Epoch [19][100/2217]\tLoss: 3.648\n",
      "Epoch [19][200/2217]\tLoss: 3.651\n",
      "Epoch [19][300/2217]\tLoss: 3.655\n",
      "Epoch [19][400/2217]\tLoss: 3.658\n",
      "Epoch [19][500/2217]\tLoss: 3.663\n",
      "Epoch [19][600/2217]\tLoss: 3.667\n",
      "Epoch [19][700/2217]\tLoss: 3.670\n",
      "Epoch [19][800/2217]\tLoss: 3.673\n",
      "Epoch [19][900/2217]\tLoss: 3.675\n",
      "Epoch [19][1000/2217]\tLoss: 3.677\n",
      "Epoch [19][1100/2217]\tLoss: 3.680\n",
      "Epoch [19][1200/2217]\tLoss: 3.683\n",
      "Epoch [19][1300/2217]\tLoss: 3.685\n",
      "Epoch [19][1400/2217]\tLoss: 3.688\n",
      "Epoch [19][1500/2217]\tLoss: 3.691\n",
      "Epoch [19][1600/2217]\tLoss: 3.693\n",
      "Epoch [19][1700/2217]\tLoss: 3.696\n",
      "Epoch [19][1800/2217]\tLoss: 3.699\n",
      "Epoch [19][1900/2217]\tLoss: 3.701\n",
      "Epoch [19][2000/2217]\tLoss: 3.703\n",
      "Epoch [19][2100/2217]\tLoss: 3.705\n",
      "Epoch [19][2200/2217]\tLoss: 3.708\n",
      "Epoch [20][0/2217]\tLoss: 3.540\n",
      "Epoch [20][100/2217]\tLoss: 3.620\n",
      "Epoch [20][200/2217]\tLoss: 3.625\n",
      "Epoch [20][300/2217]\tLoss: 3.633\n",
      "Epoch [20][400/2217]\tLoss: 3.636\n",
      "Epoch [20][500/2217]\tLoss: 3.637\n",
      "Epoch [20][600/2217]\tLoss: 3.641\n",
      "Epoch [20][700/2217]\tLoss: 3.646\n",
      "Epoch [20][800/2217]\tLoss: 3.650\n",
      "Epoch [20][900/2217]\tLoss: 3.655\n",
      "Epoch [20][1000/2217]\tLoss: 3.660\n",
      "Epoch [20][1100/2217]\tLoss: 3.661\n",
      "Epoch [20][1200/2217]\tLoss: 3.664\n",
      "Epoch [20][1300/2217]\tLoss: 3.666\n",
      "Epoch [20][1400/2217]\tLoss: 3.669\n",
      "Epoch [20][1500/2217]\tLoss: 3.672\n",
      "Epoch [20][1600/2217]\tLoss: 3.674\n",
      "Epoch [20][1700/2217]\tLoss: 3.677\n",
      "Epoch [20][1800/2217]\tLoss: 3.679\n",
      "Epoch [20][1900/2217]\tLoss: 3.681\n",
      "Epoch [20][2000/2217]\tLoss: 3.683\n",
      "Epoch [20][2100/2217]\tLoss: 3.685\n",
      "Epoch [20][2200/2217]\tLoss: 3.687\n",
      "Epoch [21][0/2217]\tLoss: 3.524\n",
      "Epoch [21][100/2217]\tLoss: 3.609\n",
      "Epoch [21][200/2217]\tLoss: 3.615\n",
      "Epoch [21][300/2217]\tLoss: 3.617\n",
      "Epoch [21][400/2217]\tLoss: 3.621\n",
      "Epoch [21][500/2217]\tLoss: 3.621\n",
      "Epoch [21][600/2217]\tLoss: 3.627\n",
      "Epoch [21][700/2217]\tLoss: 3.631\n",
      "Epoch [21][800/2217]\tLoss: 3.638\n",
      "Epoch [21][900/2217]\tLoss: 3.640\n",
      "Epoch [21][1000/2217]\tLoss: 3.643\n",
      "Epoch [21][1100/2217]\tLoss: 3.646\n",
      "Epoch [21][1200/2217]\tLoss: 3.648\n",
      "Epoch [21][1300/2217]\tLoss: 3.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21][1400/2217]\tLoss: 3.652\n",
      "Epoch [21][1500/2217]\tLoss: 3.654\n",
      "Epoch [21][1600/2217]\tLoss: 3.656\n",
      "Epoch [21][1700/2217]\tLoss: 3.657\n",
      "Epoch [21][1800/2217]\tLoss: 3.659\n",
      "Epoch [21][1900/2217]\tLoss: 3.661\n",
      "Epoch [21][2000/2217]\tLoss: 3.662\n",
      "Epoch [21][2100/2217]\tLoss: 3.664\n",
      "Epoch [21][2200/2217]\tLoss: 3.666\n",
      "Epoch [22][0/2217]\tLoss: 3.840\n",
      "Epoch [22][100/2217]\tLoss: 3.603\n",
      "Epoch [22][200/2217]\tLoss: 3.595\n",
      "Epoch [22][300/2217]\tLoss: 3.593\n",
      "Epoch [22][400/2217]\tLoss: 3.597\n",
      "Epoch [22][500/2217]\tLoss: 3.603\n",
      "Epoch [22][600/2217]\tLoss: 3.606\n",
      "Epoch [22][700/2217]\tLoss: 3.609\n",
      "Epoch [22][800/2217]\tLoss: 3.614\n",
      "Epoch [22][900/2217]\tLoss: 3.617\n",
      "Epoch [22][1000/2217]\tLoss: 3.619\n",
      "Epoch [22][1100/2217]\tLoss: 3.621\n",
      "Epoch [22][1200/2217]\tLoss: 3.623\n",
      "Epoch [22][1300/2217]\tLoss: 3.626\n",
      "Epoch [22][1400/2217]\tLoss: 3.629\n",
      "Epoch [22][1500/2217]\tLoss: 3.631\n",
      "Epoch [22][1600/2217]\tLoss: 3.633\n",
      "Epoch [22][1700/2217]\tLoss: 3.636\n",
      "Epoch [22][1800/2217]\tLoss: 3.638\n",
      "Epoch [22][1900/2217]\tLoss: 3.640\n",
      "Epoch [22][2000/2217]\tLoss: 3.642\n",
      "Epoch [22][2100/2217]\tLoss: 3.645\n",
      "Epoch [22][2200/2217]\tLoss: 3.647\n",
      "Epoch [23][0/2217]\tLoss: 3.613\n",
      "Epoch [23][100/2217]\tLoss: 3.567\n",
      "Epoch [23][200/2217]\tLoss: 3.573\n",
      "Epoch [23][300/2217]\tLoss: 3.577\n",
      "Epoch [23][400/2217]\tLoss: 3.582\n",
      "Epoch [23][500/2217]\tLoss: 3.586\n",
      "Epoch [23][600/2217]\tLoss: 3.589\n",
      "Epoch [23][700/2217]\tLoss: 3.591\n",
      "Epoch [23][800/2217]\tLoss: 3.595\n",
      "Epoch [23][900/2217]\tLoss: 3.598\n",
      "Epoch [23][1000/2217]\tLoss: 3.602\n",
      "Epoch [23][1100/2217]\tLoss: 3.605\n",
      "Epoch [23][1200/2217]\tLoss: 3.607\n",
      "Epoch [23][1300/2217]\tLoss: 3.609\n",
      "Epoch [23][1400/2217]\tLoss: 3.612\n",
      "Epoch [23][1500/2217]\tLoss: 3.615\n",
      "Epoch [23][1600/2217]\tLoss: 3.617\n",
      "Epoch [23][1700/2217]\tLoss: 3.618\n",
      "Epoch [23][1800/2217]\tLoss: 3.621\n",
      "Epoch [23][1900/2217]\tLoss: 3.623\n",
      "Epoch [23][2000/2217]\tLoss: 3.625\n",
      "Epoch [23][2100/2217]\tLoss: 3.627\n",
      "Epoch [23][2200/2217]\tLoss: 3.629\n",
      "Epoch [24][0/2217]\tLoss: 3.545\n",
      "Epoch [24][100/2217]\tLoss: 3.543\n",
      "Epoch [24][200/2217]\tLoss: 3.550\n",
      "Epoch [24][300/2217]\tLoss: 3.558\n",
      "Epoch [24][400/2217]\tLoss: 3.560\n",
      "Epoch [24][500/2217]\tLoss: 3.562\n",
      "Epoch [24][600/2217]\tLoss: 3.564\n",
      "Epoch [24][700/2217]\tLoss: 3.567\n",
      "Epoch [24][800/2217]\tLoss: 3.572\n",
      "Epoch [24][900/2217]\tLoss: 3.575\n",
      "Epoch [24][1000/2217]\tLoss: 3.579\n",
      "Epoch [24][1100/2217]\tLoss: 3.581\n",
      "Epoch [24][1200/2217]\tLoss: 3.585\n",
      "Epoch [24][1300/2217]\tLoss: 3.588\n",
      "Epoch [24][1400/2217]\tLoss: 3.591\n",
      "Epoch [24][1500/2217]\tLoss: 3.593\n",
      "Epoch [24][1600/2217]\tLoss: 3.597\n",
      "Epoch [24][1700/2217]\tLoss: 3.599\n",
      "Epoch [24][1800/2217]\tLoss: 3.601\n",
      "Epoch [24][1900/2217]\tLoss: 3.602\n",
      "Epoch [24][2000/2217]\tLoss: 3.604\n",
      "Epoch [24][2100/2217]\tLoss: 3.606\n",
      "Epoch [24][2200/2217]\tLoss: 3.609\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    \n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "df889b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint_24.pth.tar')\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e9f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are you\n",
      "Maximum Reply Length: 25\n",
      "i dont know yeah\n",
      "Question: Where do you live\n",
      "Maximum Reply Length: 10\n",
      "i dont know yeah\n",
      "Question: b\n",
      "Maximum Reply Length: 1\n",
      "\n",
      "Question: catherine says what\n",
      "Maximum Reply Length: 10\n",
      "i dont know yeah\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    max_len = input(\"Maximum Reply Length: \")\n",
    "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83ed24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
